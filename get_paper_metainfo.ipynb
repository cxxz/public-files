{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86df7673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import arxiv\n",
    "import requests\n",
    "import random\n",
    "from time import sleep\n",
    "from scholarly import scholarly\n",
    "from scrapeless import ScrapelessClient\n",
    "from serpapi.google_search import GoogleSearch\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "scrapeless_token = os.getenv(\"SCRAPELESS_API_KEY\")\n",
    "scrapeless = ScrapelessClient(api_key=scrapeless_token)\n",
    "\n",
    "# Replace with your own SerpAPI API key\n",
    "serpapi_token = os.getenv(\"SERPAPI_KEY\")\n",
    "# print(serpapi_key)\n",
    "\n",
    "def extract_venue(citation_snippet):\n",
    "    # This regex looks for:\n",
    "    # - A paper title enclosed in quotes (non-greedily with \".*?\")\n",
    "    # - Then optional whitespace\n",
    "    # - Then captures one or more non-digit characters (the venue)\n",
    "    #   until it sees a digit, possibly after some spaces or colons.\n",
    "    pattern = r'\".*?\"\\s*([^0-9]+?)(?=[\\s:]*\\d)'\n",
    "    \n",
    "    match = re.search(pattern, citation_snippet)\n",
    "    if match:\n",
    "        venue = match.group(1).strip()\n",
    "        # For cases like arXiv, remove a trailing 'arXiv:' if present.\n",
    "        if venue and 'arxiv' in venue.lower():\n",
    "            venue = 'arXiv'\n",
    "        if len(venue) < 3:\n",
    "            return None\n",
    "        return venue\n",
    "    return None\n",
    "\n",
    "def get_author_info_by_gs_id(author_id):\n",
    "    \"\"\"\n",
    "    Fetches author information from Google Scholar using the author ID.\n",
    "    Args:\n",
    "        author_id (str): The Google Scholar author ID.\n",
    "    Returns:\n",
    "        dict: Author information including name, affiliation, and publications.\n",
    "    \"\"\"\n",
    "    sleep(random.randint(1, 3))\n",
    "    try:\n",
    "        author_info = scholarly.search_author_id(author_id)\n",
    "\n",
    "        if author_info:\n",
    "            author_name = author_info.get('name', None)\n",
    "            author_affiliation = author_info.get('affiliation', None)\n",
    "            if author_affiliation and \",\" in author_affiliation:\n",
    "                author_affiliation = author_affiliation.split(\",\")[-1]\n",
    "            return {\n",
    "                'name': author_name,\n",
    "                'affiliation': author_affiliation\n",
    "            }\n",
    "        else:\n",
    "            print(\"No author information found.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching author info: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_author_info_serpapi(author_id):\n",
    "    \"\"\"\n",
    "    Search for an author by their Google Scholar ID and retrieve their details.\n",
    "    \"\"\"\n",
    "    # Define the parameters for the search\n",
    "    params = {\n",
    "        \"api_key\": serpapi_token,\n",
    "        \"engine\": \"google_scholar_author\",\n",
    "        \"author_id\" : author_id\n",
    "    }\n",
    "\n",
    "    # Initialize and execute the search\n",
    "    author_search = GoogleSearch(params)\n",
    "    return author_search.get_dict()\n",
    "\n",
    "def get_paper_info_serpapi(paper_title):\n",
    "    \"\"\"\n",
    "    Search for a paper by title and retrieve its details using SerpAPI.\n",
    "    \"\"\"\n",
    "    # Define the parameters for the search\n",
    "\n",
    "    params = {\n",
    "        \"api_key\": serpapi_token,\n",
    "        \"engine\": \"google_scholar\",  # Use the Google Scholar engine\n",
    "        \"q\": paper_title,            # Query the paper title\n",
    "    }\n",
    "\n",
    "    # Initialize the search\n",
    "    search = GoogleSearch(params)\n",
    "\n",
    "    # Get the full response as a Python dictionary\n",
    "    try:\n",
    "      result = search.get_dict()\n",
    "      if 'organic_results' in result or 'organic_result' in result:\n",
    "          organic_results = result['organic_results'] if 'organic_results' in result else result['organic_result']\n",
    "          if len(organic_results) > 0:\n",
    "            return organic_results[0]\n",
    "    except Exception as e:\n",
    "      print(f\"Error fetching {paper_title}:\\n{e}\")\n",
    "      return None\n",
    "    \n",
    "    print(\"No organic results found.\")\n",
    "    return None\n",
    "\n",
    "def get_paper_info_scrapeless(paper_title, max_retries=3):\n",
    "  \"\"\"\n",
    "  Fetches paper information from Google Scholar using Scrapeless API.\n",
    "  Args:\n",
    "    paper_title (str): The title of the paper to search for.\n",
    "    max_retries (int): Maximum number of retries for the API request.\n",
    "  Returns:\n",
    "    dict: The first organic result from Google Scholar.\n",
    "  \"\"\"\n",
    "  actor = \"scraper.google.scholar\"\n",
    "  input_data = {\n",
    "    \"q\" : paper_title,\n",
    "  }\n",
    "  try:\n",
    "    for i in range(max_retries):\n",
    "      # Make the API request\n",
    "      result = scrapeless.scraper(actor, input=input_data)\n",
    "      if 'organic_results' in result or 'organic_result' in result:\n",
    "        organic_results = result['organic_results'] if 'organic_results' in result else result['organic_result']\n",
    "        if len(organic_results) > 0:\n",
    "          return organic_results[0]\n",
    "        else:\n",
    "          print(\"No organic results found.\")\n",
    "          return None\n",
    "      else:\n",
    "        print(f\"No organic results found in the response at iteration {i + 1}.\")\n",
    "        if i < max_retries - 1:\n",
    "          print(\"Retrying...\")\n",
    "          sleep(3)\n",
    "  except Exception as e:\n",
    "    print(f\"Error fetching {paper_title}:\\n{e}\")\n",
    "    return None\n",
    "\n",
    "def get_paper_citations_scrapeless(result_id, debug = False):\n",
    "  \"\"\"\n",
    "  Fetches citation information from Google Scholar using Scrapeless API.\n",
    "  Args:\n",
    "    result_id (str): The ID of the paper result.\n",
    "  Returns:\n",
    "    dict: Citation information including title, authors, and abstract.\n",
    "  \"\"\"\n",
    "  actor = \"scraper.google.scholar.cite\"\n",
    "  input_data = {\n",
    "    \"q\": result_id,\n",
    "  }\n",
    "  venue = None\n",
    "  bibtex_link = None\n",
    "  try:\n",
    "    # Make the API request\n",
    "    result = scrapeless.scraper(actor, input=input_data)\n",
    "    if 'citations' in result:\n",
    "      citations = result['citations']\n",
    "      for c in citations:\n",
    "        if debug:\n",
    "          print(c)\n",
    "        if c.get(\"title\") == \"MLA\":\n",
    "          citation_snippet = c.get(\"snippet\", None)\n",
    "          if citation_snippet:\n",
    "            venue = extract_venue(citation_snippet)\n",
    "\n",
    "    if 'links' in result:\n",
    "      bib_links = result['links']\n",
    "      for b in bib_links:\n",
    "        if debug:\n",
    "          print(b)\n",
    "        if b.get(\"name\").lower() == \"bibtex\":\n",
    "          bibtex_link = b.get(\"link\")\n",
    "\n",
    "  except Exception as e:\n",
    "    print(f\"Error fetching citations:\\n{e}\")\n",
    "    return None\n",
    "  \n",
    "  if venue and 'arxiv' in venue:\n",
    "    venue = 'arxiv'\n",
    "    \n",
    "  return {\n",
    "    'venue': venue,\n",
    "    'bibtex_link': bibtex_link\n",
    "  }\n",
    "  \n",
    "def extract_paper_info(scrapeless_result):\n",
    "  \"\"\"\n",
    "  Extracts relevant information from the Scrapeless API result.\n",
    "  Args:\n",
    "    scrapeless_result (dict): The result from Scrapeless API.\n",
    "  Returns:\n",
    "    dict: Extracted information including title, authors, and abstract.\n",
    "  \"\"\"\n",
    "  title = scrapeless_result.get('title')\n",
    "  authors = scrapeless_result.get('authors')\n",
    "  abstract = scrapeless_result.get('abstract')\n",
    "  \n",
    "  return {\n",
    "    'title': title,\n",
    "    'authors': authors,\n",
    "    'abstract': abstract\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "def get_abstract_from_openview(forum_id):\n",
    "    \"\"\"\n",
    "    Fetch the abstract from OpenReview using the forum ID.\n",
    "    \"\"\"\n",
    "    # OpenReview API endpoint to get note details by forum id\n",
    "    url = f\"https://api2.openreview.net/notes?forum={forum_id}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        notes = data.get('notes', [])\n",
    "        if notes:\n",
    "            # Extract the abstract from the note content\n",
    "            abstract = notes[0]['content'].get('abstract', None)\n",
    "            if abstract:\n",
    "                return abstract['value']\n",
    "            else:\n",
    "                print(\"Abstract not found in the note content.\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"No notes found in the response.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from OpenReview API. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def get_abstract_from_arxiv_link(arxiv_link):\n",
    "    # Extract the arXiv ID from the link\n",
    "    # ArXiv links typically have the format: https://arxiv.org/abs/XXXX.XXXXX\n",
    "    arxiv_id = arxiv_link.split(\"/\")[-1]\n",
    "    \n",
    "    # Create a client to interact with the arXiv API\n",
    "    client = arxiv.Client()\n",
    "    \n",
    "    # Search for the paper by ID\n",
    "    search = arxiv.Search(id_list=[arxiv_id])\n",
    "    \n",
    "    # Get the first result (there should be only one since we're searching by ID)\n",
    "    paper = next(client.results(search))\n",
    "    \n",
    "    # Return the abstract\n",
    "    return paper.summary\n",
    "\n",
    "\n",
    "def get_paper_metadata(paper_title):\n",
    "    \"\"\"\n",
    "    Search for a paper by title and retrieve its details.\n",
    "    \"\"\"\n",
    "    # add a random delay between 1 second and 3 seconds\n",
    "    sleep(random.randint(1, 3))\n",
    "    # if no space in paper_title, and more than one \"_\" replace \"_\" with \" \"\n",
    "    if \" \" not in paper_title:\n",
    "        if paper_title.count(\"_\") > 0:\n",
    "            paper_title = paper_title.replace(\"_\", \" \")\n",
    "        elif paper_title.count(\"-\") > 2:\n",
    "            paper_title = paper_title.replace(\"-\", \" \")\n",
    "        # if paper_title is like \"CommunicativeAgentForSoftwareDevelopement\", then add space between each word so it comes\n",
    "        # out as \"Communicative Agent For Software Developement\"\n",
    "        elif paper_title[0].isupper() and paper_title[1].islower():\n",
    "            paper_title = re.sub(r\"(?<!^)(?=[A-Z])\", \" \", paper_title)\n",
    "            print(f\"Reformated paper title: {paper_title}\")\n",
    "\n",
    "    try:\n",
    "        paper_info = get_paper_info_scrapeless(paper_title)\n",
    "        # paper_info = scholarly.search_single_pub(paper_title)\n",
    "        # search_query = scholarly.search_pubs(paper_title)\n",
    "        # pub = next(search_query, None)\n",
    "        # if pub:\n",
    "        #     paper_info = scholarly.fill(pub)\n",
    "        # else:\n",
    "        #     print(\"No publication found\")\n",
    "        #     return None\n",
    "    except Exception as e:\n",
    "        print(f\"Searching {paper_title} got an error: {e}\")\n",
    "        return None\n",
    "    \"\"\"Example paper_info:\n",
    "{'position': 1,\n",
    " 'title': 'Data efficient evaluation of large language models and text-to-image models via adaptive sampling',\n",
    " 'result_id': 'X8cSSWkY13EJ',\n",
    " 'link': 'https://arxiv.org/abs/2406.15527',\n",
    " 'publication_info': {'summary': 'C Xu, G Saranathan, MP Alam, A Shah, J Lim…\\xa0- arXiv preprint arXiv\\xa0…, 2024 - arxiv.org',\n",
    "  'authors': [{'name': 'C Xu',\n",
    "    'link': 'http://https://scholar.google.com/citations?user=B8WA2XsAAAAJ&hl=en&oi=sra',\n",
    "    'author_id': 'B8WA2XsAAAAJ'},\n",
    "   {'name': 'G Saranathan',\n",
    "    'link': 'http://https://scholar.google.com/citations?user=haF4QnMAAAAJ&hl=en&oi=sra',\n",
    "    'author_id': 'haF4QnMAAAAJ'},\n",
    "   {'name': 'MP Alam',\n",
    "    'link': 'http://https://scholar.google.com/citations?user=V06zkkwAAAAJ&hl=en&oi=sra',\n",
    "    'author_id': 'V06zkkwAAAAJ'},\n",
    "   {'name': 'A Shah',\n",
    "    'link': 'http://https://scholar.google.com/citations?user=zMszxvYAAAAJ&hl=en&oi=sra',\n",
    "    'author_id': 'zMszxvYAAAAJ'}]},\n",
    " 'resources': [{'title': 'arxiv.org',\n",
    "   'file_format': '[PDF]',\n",
    "   'link': 'https://arxiv.org/pdf/2406.15527'}],\n",
    " 'inline_links': {'cited_by': {'total': 6,\n",
    "   'link': 'https://scholar.google.com',\n",
    "   'cites_id': ''}}}\n",
    "    \"\"\"\n",
    "    if not paper_info:\n",
    "        print(\"Paper not found with Scrapeless API. Retry with SerpAPI.\")\n",
    "        paper_info = get_paper_info_serpapi(paper_title)\n",
    "    if not paper_info:\n",
    "        print(\"Paper not found with SerpAPI.\")\n",
    "        return None\n",
    "    \n",
    "    title = paper_info.get('title', None)\n",
    "    result_id = paper_info.get('result_id', None)\n",
    "    pub_url = paper_info.get('link', None)\n",
    "    authors = paper_info.get('publication_info', {}).get('authors', [])\n",
    "    author_names = None\n",
    "    author_ids = None\n",
    "    if authors:\n",
    "        author_names = [author['name'] for author in authors]\n",
    "        author_ids = [author['author_id'] for author in authors]\n",
    "        # print(f\"author_ids: {author_ids}\")\n",
    "    \n",
    "    resource_link = None\n",
    "    if 'resources' in paper_info:\n",
    "      resource = paper_info['resources'][0]\n",
    "      if 'link' in resource:\n",
    "        resource_link = resource['link']\n",
    "        # print(f\"resource_link: {resource_link}\")\n",
    "\n",
    "    venue = None\n",
    "    bibtex_link = None\n",
    "    if result_id:\n",
    "        citations = get_paper_citations_scrapeless(result_id)\n",
    "        if citations:\n",
    "            venue = citations.get('venue', None)\n",
    "            bibtex_link = citations.get('bibtex_link', None)\n",
    "    \n",
    "        # print(f\"Title: {title}\")\n",
    "        # print(f\"Author_ids: {author_ids}\")\n",
    "        # print(f\"Publication Year: {pub_year}\")\n",
    "        # print(f\"Venue: {venue}\")\n",
    "        # print(f\"Abstract: {abstract}\")\n",
    "        # print(f\"Public URL: {pub_url}\")\n",
    "    else:\n",
    "        print(\"No bibliographic information available\")\n",
    "    \n",
    "    first_author_name = None\n",
    "    first_author_affiliation = None\n",
    "    if author_ids:\n",
    "        first_author_id = author_ids[0]\n",
    "        if len(first_author_id) > 5:\n",
    "            # print(f\"First Author ID: {first_author_id}\")\n",
    "            author_info = get_author_info_by_gs_id(first_author_id)\n",
    "            if author_info:\n",
    "                first_author_name = author_info.get('name', None)\n",
    "                first_author_affiliation = author_info.get('affiliation', None)\n",
    "            else:\n",
    "                print(f\"No author information found for ID: {first_author_id}\")\n",
    "\n",
    "    abstract = None\n",
    "    code_url = None\n",
    "\n",
    "    if pub_url:\n",
    "      if 'arxiv' in pub_url:\n",
    "          abstract = get_abstract_from_arxiv_link(pub_url)\n",
    "          # print(f\"Abstract: {abstract}\")\n",
    "      elif 'openreview' in pub_url:\n",
    "          forum_id = pub_url.split(\"/\")[-1]\n",
    "          abstract = get_abstract_from_openview(forum_id)\n",
    "          # print(f\"Abstract: {abstract}\")\n",
    "\n",
    "    if abstract and 'github.com/' in abstract:\n",
    "        code_url = \"https://github.com/\" + abstract.split(\"github.com/\")[1].split(' ')[0].split('.')[0]\n",
    "        # print(f\"Code URL: {code_url}\")\n",
    "\n",
    "    results_json = {\n",
    "        \"title\": title,\n",
    "        \"author_names\": author_names,\n",
    "        \"author_ids\": author_ids,\n",
    "        \"public_url\": pub_url,\n",
    "        \"code_url\": code_url,\n",
    "        \"venue\": venue,\n",
    "        \"result_id\": result_id,\n",
    "        \"resource_link\": resource_link,\n",
    "        \"bibtex_link\": bibtex_link,\n",
    "        \"first_author_name\": first_author_name,\n",
    "        \"first_author_affiliation\": first_author_affiliation,\n",
    "        \"abstract\": abstract\n",
    "    }\n",
    "    return results_json\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7188ae03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No organic results found in the response at iteration 1.\n",
      "Retrying...\n",
      "No organic results found in the response at iteration 2.\n",
      "Retrying...\n",
      "No organic results found in the response at iteration 3.\n",
      "Paper not found with Scrapeless API. Retry with SerpAPI.\n",
      "{'abstract': 'Evaluating LLMs and text-to-image models is a computationally '\n",
      "             'intensive task\\n'\n",
      "             'often overlooked. Efficient evaluation is crucial for '\n",
      "             'understanding the diverse\\n'\n",
      "             'capabilities of these models and enabling comparisons across a '\n",
      "             'growing number\\n'\n",
      "             'of new models and benchmarks. To address this, we introduce '\n",
      "             'SubLIME, a\\n'\n",
      "             'data-efficient evaluation framework that employs adaptive '\n",
      "             'sampling techniques,\\n'\n",
      "             'such as clustering and quality-based methods, to create '\n",
      "             'representative subsets\\n'\n",
      "             'of benchmarks. Our approach ensures statistically aligned model '\n",
      "             'rankings\\n'\n",
      "             'compared to full datasets, evidenced by high Pearson correlation '\n",
      "             'coefficients.\\n'\n",
      "             'Empirical analysis across six NLP benchmarks reveals that: (1) '\n",
      "             'quality-based\\n'\n",
      "             'sampling consistently achieves strong correlations (0.85 to '\n",
      "             '0.95) with full\\n'\n",
      "             'datasets at a 10\\\\% sampling rate such as Quality SE and Quality '\n",
      "             'CPD (2)\\n'\n",
      "             'clustering methods excel in specific benchmarks such as MMLU (3) '\n",
      "             'no single\\n'\n",
      "             'method universally outperforms others across all metrics. '\n",
      "             'Extending this\\n'\n",
      "             'framework, we leverage the HEIM leaderboard to cover 25 '\n",
      "             'text-to-image models on\\n'\n",
      "             '17 different benchmarks. SubLIME dynamically selects the optimal '\n",
      "             'technique for\\n'\n",
      "             'each benchmark, significantly reducing evaluation costs while '\n",
      "             'preserving\\n'\n",
      "             'ranking integrity and score distribution. Notably, a minimal '\n",
      "             'sampling rate of\\n'\n",
      "             '1% proves effective for benchmarks like MMLU. Additionally, we '\n",
      "             'demonstrate that\\n'\n",
      "             'employing difficulty-based sampling to target more challenging '\n",
      "             'benchmark\\n'\n",
      "             'segments enhances model differentiation with broader score '\n",
      "             'distributions. We\\n'\n",
      "             'also combine semantic search, tool use, and GPT-4 review to '\n",
      "             'identify redundancy\\n'\n",
      "             'across benchmarks within specific LLM categories, such as coding '\n",
      "             'benchmarks.\\n'\n",
      "             'This allows us to further reduce the number of samples needed to '\n",
      "             'maintain\\n'\n",
      "             'targeted rank preservation. Overall, SubLIME offers a versatile '\n",
      "             'and\\n'\n",
      "             'cost-effective solution for the robust evaluation of LLMs and '\n",
      "             'text-to-image\\n'\n",
      "             'models.',\n",
      " 'author_ids': ['B8WA2XsAAAAJ',\n",
      "                'haF4QnMAAAAJ',\n",
      "                'V06zkkwAAAAJ',\n",
      "                'zMszxvYAAAAJ',\n",
      "                '0T8d4TIAAAAJ'],\n",
      " 'author_names': ['C Xu', 'G Saranathan', 'MP Alam', 'A Shah', 'J Lim'],\n",
      " 'bibtex_link': None,\n",
      " 'code_url': None,\n",
      " 'first_author_affiliation': ' Hewlett Packard Labs',\n",
      " 'first_author_name': 'Cong Xu',\n",
      " 'public_url': 'https://arxiv.org/abs/2406.15527',\n",
      " 'resource_link': 'https://arxiv.org/pdf/2406.15527',\n",
      " 'result_id': 'X8cSSWkY13EJ',\n",
      " 'title': 'Data efficient evaluation of large language models and '\n",
      "          'text-to-image models via adaptive sampling',\n",
      " 'venue': None}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "title = \"Data efficient evaluation of large language models and text-to-image models via adaptive sampling\"\n",
    "result = get_paper_metadata(title)\n",
    "pprint(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
